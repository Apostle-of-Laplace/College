{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2396dd03-7f7b-4566-8264-8fe3b12ececd",
   "metadata": {},
   "source": [
    "# CBEE 213 Midterm Group Project GROUP COMPONENT  \n",
    "Samuel Perkins  \n",
    "Rachel Strelow  \n",
    "Trey Stephens-Cherry  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "463492c2-6b4a-4a3d-b5ab-759051b7551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "import os\n",
    "df = pd.read_excel(\"Biocharcoals and Soil Nitrates_SIMPLIFIED.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13bbb1-2223-4492-b886-d54fd0c1d75e",
   "metadata": {},
   "source": [
    "### Part 5: Comparing Confidence intervals\n",
    "\n",
    "Here is the confidence intervals calculated in the individual components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40e4cc06-68ff-43df-a606-d323aebed60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locating file path; note that your data file must be in the same directory as this file\n",
    "def path_finder(file_name):\n",
    "    \n",
    "    directory = os.path.dirname('__file__')\n",
    "    file_directory = os.path.join(directory, file_name)\n",
    "    \n",
    "    # print(f'Found directory: {file_directory}')\n",
    "    \n",
    "    return file_directory\n",
    "\n",
    "\n",
    "# Reading the found file\n",
    "found_data = pd.read_excel(\"Biocharcoals and Soil Nitrates_SIMPLIFIED.xlsx\")\n",
    "\n",
    "\n",
    "# This function creates a dictionary that groups the data by Soil and Biochar\n",
    "# It works by iterating through all rows in the DataFrame, and creating a key\n",
    "# based on the row contents.\n",
    "def data_sorter(data):\n",
    "    \n",
    "    grouped_data = {}\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        \n",
    "        soil_type = row['Soil']\n",
    "        biochar_content = row['Biochar']\n",
    "        \n",
    "        key = (soil_type, biochar_content)\n",
    "    \n",
    "        if key not in grouped_data:\n",
    "            grouped_data[key] = []\n",
    "            \n",
    "        grouped_data[key].append(row.to_dict())\n",
    "        \n",
    "    return grouped_data\n",
    "\n",
    "\n",
    "# Finds the relevant statistics data, mean/median/standard dev/... etc\n",
    "def data_analyzer(data):\n",
    "    \n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "    \n",
    "    analysis_results = {}\n",
    "    \n",
    "    for key, rows in data.items():\n",
    "        \n",
    "        soil_type, biochar_content = key\n",
    "        key = (soil_type, biochar_content if not pd.isna(biochar_content) else 'Unknown')\n",
    "\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        stats = {\n",
    "            'Average FractionCO2': df['FractionCO2'].mean(skipna=True),\n",
    "            'Median FractionCO2': df['FractionCO2'].median(skipna=True),\n",
    "            'Std FractionCO2': df['FractionCO2'].std(skipna=True),\n",
    "            'Average Grav': df['Grav'].mean(skipna=True),\n",
    "            'Median Grav': df['Grav'].median(skipna=True),\n",
    "            'Std Grav': df['Grav'].std(skipna=True),\n",
    "            'Sample Size': len(df)\n",
    "        }\n",
    "        \n",
    "        stats = {k: float(v) if isinstance(v, np.float64) else v for k, v in stats.items()}\n",
    "        \n",
    "        analysis_results[key] = stats\n",
    "        \n",
    "    return analysis_results\n",
    "\n",
    "\n",
    "# Formats the analyzed data into a cleaner output that is more legible\n",
    "def data_formatter(data):\n",
    "    \n",
    "    formatted_data = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "    formatted_data.reset_index(inplace=True)\n",
    "\n",
    "    if 'level_0' in formatted_data.columns and 'level_1' in formatted_data.columns:\n",
    "        formatted_data['Group'] = formatted_data['level_0'].astype(str) + \"-\" + formatted_data['level_1'].astype(str)\n",
    "        formatted_data.drop(['level_0', 'level_1'], axis=1, inplace=True)\n",
    "    else:\n",
    "        print(\"Expected 'level_0' and 'level_1' columns not found. Current columns:\", formatted_data.columns)\n",
    "   \n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "# Calculates the confidence interval for a given column\n",
    "def calculate_confidence_interval(data, column, confidence=0.95):\n",
    "    \n",
    "    if column not in data.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in the dataset.\")\n",
    "\n",
    "    mean = data[column].mean()\n",
    "    std_dev = data[column].std()\n",
    "    n = len(data[column])\n",
    "\n",
    "    if n < 2:\n",
    "        raise ValueError('Not enough data points to calculate a confidence interval.')\n",
    "    \n",
    "    t_critical = t.ppf((1 + confidence) / 2, df=n - 1)\n",
    "\n",
    "    margin_of_error = t_critical * (std_dev / (n ** 0.5))\n",
    "\n",
    "    lower_bound = float(mean - margin_of_error)\n",
    "    upper_bound = float(mean + margin_of_error)\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "\n",
    "# Calculates the confidence interval for a single replicate of a specific treatment\n",
    "def single_replicate_confidence_interval(data, soil, biochar, \n",
    "                                         replicate, column, \n",
    "                                         confidence=0.95):\n",
    "    \n",
    "    filtered_data = data[\n",
    "        (data['Soil'] == soil) &\n",
    "        (data['Biochar'] == biochar) &\n",
    "        (data['Replicate'] == replicate)\n",
    "    ]\n",
    "\n",
    "    return calculate_confidence_interval(filtered_data, column, confidence)\n",
    "\n",
    "\n",
    "# Confidence interval for all replicates\n",
    "def combined_replicates_confidence_interval(data, soil, \n",
    "                                            biochar, column, \n",
    "                                            confidence=0.95):\n",
    "\n",
    "    filtered_data = data[\n",
    "        (data['Soil'] == soil) &\n",
    "        (data['Biochar'] == biochar)\n",
    "    ]\n",
    "\n",
    "    return calculate_confidence_interval(filtered_data, column, confidence)\n",
    "\n",
    "\n",
    "# Function to calculate the probability of samples being within the confidence interval\n",
    "def test_samples_within_confidence_interval(data, column, soil, \n",
    "                                            biochar, confidence=0.95,\n",
    "                                            sample_size=None, \n",
    "                                            iterations=None):\n",
    " \n",
    "    filtered_data = data[\n",
    "                (data['Soil'] == soil) &\n",
    "                (data['Biochar'] == biochar)\n",
    "                ]\n",
    "    \n",
    "    if column not in filtered_data.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in the dataset.\")\n",
    "\n",
    "    lower_bound, upper_bound = calculate_confidence_interval(filtered_data, column, confidence)\n",
    "\n",
    "    within_interval_count = 0\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        sample = filtered_data[column].dropna().sample(n=sample_size, replace=True)\n",
    "\n",
    "        sample_mean = sample.mean()\n",
    "\n",
    "        if lower_bound <= sample_mean <= upper_bound:\n",
    "            within_interval_count += 1\n",
    "\n",
    "    probability = within_interval_count / iterations\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a9fd5c5-976d-451c-96b4-56f6d0c7b1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence interval analysis for Temp\n",
      "95% Confidence Interval for Replicate 1 of Chehalis-350.0: [24.7848 25.4518]\n",
      "Probability of a sample being within the confidence interval: 0.52%\n",
      "-\n",
      "95% Confidence Interval for Replicate 2 of Chehalis-350.0: [24.7848 25.4518]\n",
      "Probability of a sample being within the confidence interval: 0.52%\n",
      "-\n",
      "95% Confidence Interval for Replicate 3 of Chehalis-350.0: [24.7848 25.4518]\n",
      "Probability of a sample being within the confidence interval: 0.52%\n",
      "-\n",
      "95% Confidence Interval for All Replicates Combined: [24.94 25.29]\n",
      "-------------------------------------------------------------\n",
      "Confidence interval analysis for DeltaCO2\n",
      "95% Confidence Interval for Replicate 1 of Chehalis-350.0: [0.0065 0.0129]\n",
      "Probability of a sample being within the confidence interval: 0.53%\n",
      "-\n",
      "95% Confidence Interval for Replicate 2 of Chehalis-350.0: [0.0067 0.014 ]\n",
      "Probability of a sample being within the confidence interval: 0.53%\n",
      "-\n",
      "95% Confidence Interval for Replicate 3 of Chehalis-350.0: [0.007 0.014]\n",
      "Probability of a sample being within the confidence interval: 0.53%\n",
      "-\n",
      "95% Confidence Interval for All Replicates Combined: [0.01 0.01]\n",
      "-------------------------------------------------------------\n",
      "Confidence interval analysis for FractionCO2\n",
      "95% Confidence Interval for Replicate 1 of Chehalis-350.0: [0.1339 0.405 ]\n",
      "Probability of a sample being within the confidence interval: 0.51%\n",
      "-\n",
      "95% Confidence Interval for Replicate 2 of Chehalis-350.0: [0.1247 0.3874]\n",
      "Probability of a sample being within the confidence interval: 0.52%\n",
      "-\n",
      "95% Confidence Interval for Replicate 3 of Chehalis-350.0: [0.1259 0.3878]\n",
      "Probability of a sample being within the confidence interval: 0.52%\n",
      "-\n",
      "95% Confidence Interval for All Replicates Combined: [0.19 0.33]\n",
      "-------------------------------------------------------------\n",
      "Confidence interval analysis for Grav\n",
      "95% Confidence Interval for Replicate 1 of Chehalis-350.0: [0.4147 0.4478]\n",
      "Probability of a sample being within the confidence interval: 0.48%\n",
      "-\n",
      "95% Confidence Interval for Replicate 2 of Chehalis-350.0: [0.4158 0.4506]\n",
      "Probability of a sample being within the confidence interval: 0.49%\n",
      "-\n",
      "95% Confidence Interval for Replicate 3 of Chehalis-350.0: [0.4107 0.4469]\n",
      "Probability of a sample being within the confidence interval: 0.48%\n",
      "-\n",
      "95% Confidence Interval for All Replicates Combined: [0.42 0.44]\n",
      "-------------------------------------------------------------\n",
      "Confidence interval analysis for Nitrate\n",
      "95% Confidence Interval for Replicate 1 of Chehalis-350.0: [12.6729 42.8994]\n",
      "Probability of a sample being within the confidence interval: 0.51%\n",
      "-\n",
      "95% Confidence Interval for Replicate 2 of Chehalis-350.0: [ 9.8597 42.2147]\n",
      "Probability of a sample being within the confidence interval: 0.51%\n",
      "-\n",
      "95% Confidence Interval for Replicate 3 of Chehalis-350.0: [10.1921 45.0827]\n",
      "Probability of a sample being within the confidence interval: 0.51%\n",
      "-\n",
      "95% Confidence Interval for All Replicates Combined: [18.73 35.58]\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "soil_of_interest = \"Chehalis\"\n",
    "biochar_amount_of_interest = 350\n",
    "i = [\"Temp\", \"DeltaCO2\", \"FractionCO2\", \"Grav\", \"Nitrate\"]\n",
    "for data_point_of_interest in i:\n",
    "    print(f\"Confidence interval analysis for {data_point_of_interest}\")\n",
    "    for R in [1, 2, 3]:\n",
    "        try:\n",
    "            biochar_amount_of_interest = float(biochar_amount_of_interest)\n",
    "        except ValueError:\n",
    "            print('Error, biochar must be a number')\n",
    "            exit()\n",
    "        \n",
    "        if data_point_of_interest not in found_data.columns:\n",
    "            print(f\"Error: Data point '{data_point_of_interest}' not found in the dataset.\")\n",
    "            exit()\n",
    "        \n",
    "        try:\n",
    "            ci_single = single_replicate_confidence_interval( \n",
    "                data=found_data, soil=soil_of_interest, biochar=biochar_amount_of_interest,\n",
    "                replicate=R, column=data_point_of_interest)\n",
    "        \n",
    "            # Gets the confidence interval for all replicates\n",
    "            ci_combined = combined_replicates_confidence_interval(\n",
    "                data=found_data, soil=soil_of_interest,\n",
    "                biochar=biochar_amount_of_interest, column=data_point_of_interest)\n",
    "        \n",
    "            # Finding the probability that the sample average lies within the confidence interval\n",
    "            probability = test_samples_within_confidence_interval(\n",
    "                data=found_data, column=data_point_of_interest, soil=soil_of_interest,\n",
    "                biochar=biochar_amount_of_interest, confidence=0.95,\n",
    "                sample_size=5, iterations=10000\n",
    "            )\n",
    "        \n",
    "            #print(\"\\n--- Confidence Interval Results ---\")\n",
    "            print(f\"95% Confidence Interval for Replicate {R} of {soil_of_interest}-{biochar_amount_of_interest}: {np.round(ci_single, 4)}\")\n",
    "            print(f\"Probability of a sample being within the confidence interval: {probability:.2f}%\")\n",
    "            print(\"-\")\n",
    "        except Exception as e:\n",
    "            print(f'Error: {e}')\n",
    "    print(f\"95% Confidence Interval for All Replicates Combined: {np.round(ci_combined, 2)}\")\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c6896-a100-49f2-a83f-275325b95d6b",
   "metadata": {},
   "source": [
    "1. What do these confidence intervals calculated in 3a. tell you?  \n",
    "They tell us the interval at which a random sample's mean will be included in 95% of the time. \n",
    "\n",
    "2. In a markdown cell, explain broadly how each group member’s single-replicate confidence intervals within the “Chehalis.500” treatment may have differed. How did the single-replicates confidence intervals broadly compare to the all-replicates confidence intervals? Which would you use when communicating these data?  \n",
    "For most categories, each replicate's CIs are pretty close together, and their differences are probably due to the natrual variability within the data. The single-replicate CIs are also similar to the all-replicate CIs, although the all-replicate CIs seem to be slightly wider. When communicating data, the all-replicate CIs would be used since they communicate the most well-rounded and complete data. \n",
    "\n",
    "3. Count how many measurements fall outside of each confidence inteval?  \n",
    "   The probability measurements in the output above gives the answer for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f99bf0c9-5ddc-47bb-922a-3193638ab02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 12 total values for Temp, there are 7 values that are outside the confidence interval.\n",
      "Out of 12 total values for DeltaCO2, there are 7 values that are outside the confidence interval.\n",
      "Out of 12 total values for FractionCO2, there are 6 values that are outside the confidence interval.\n",
      "Out of 10 total values for Grav, there are 1 values that are outside the confidence interval.\n",
      "Out of 10 total values for Nitrate, there are 5 values that are outside the confidence interval.\n"
     ]
    }
   ],
   "source": [
    "# Count how many of the values fall outside of the confidence intervals\n",
    "\n",
    "# ~~~~~ NOTE: I do not know wether this section refers to just the overall measurements or the measurements for each replicate ~~~~~\n",
    "\n",
    "# Replicate 1\n",
    "# Index to isolate the Replicate 1 of Chehalis.500 data\n",
    "idx = ( (df[\"Treatment\"] == \"Chehalis.500\") & (df[\"Replicate\"] == 1) )\n",
    "# This loops through each type of measurement and prints a summary about the data's relation to the CIs\n",
    "for i in R_key:\n",
    "    CI_index = R_key.index(i)\n",
    "    # NaN values always return False when comparing to a number, so by checking for numbers\n",
    "    # below the low CI and above the high CI, false positives from the NaN cells can be avoided\n",
    "    below_CI = sum(df[idx][i] < R1_CI_low[CI_index])\n",
    "    above_CI = sum(df[idx][i] > R1_CI_high[CI_index])\n",
    "    outside_CI = below_CI + above_CI\n",
    "    value_count = df[idx][i].count() # this counts the number of non-NaN values in the set\n",
    "    print(f\"Out of {value_count} total values for {i}, there are {outside_CI} values that are outside the confidence interval.\")\n",
    "\n",
    "\n",
    "# Replicate 2\n",
    "\n",
    "\n",
    "\n",
    "# Replicate 3\n",
    "\n",
    "\n",
    "\n",
    "# Overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e8ffe-179e-4685-8510-e1303f59dea6",
   "metadata": {},
   "source": [
    "### Part 6\n",
    "1. In a markdown cell, write your relationship prediction in sentence form  \n",
    "   Hypothesis one: The Willamette-350 treatment will have grater Fractional CO2 than the Chehalis-350 treatment.  \n",
    "   Hypothesis two: The Willamette-500 treatment will have a differnent Nitrate content Willamette-700 treatment.  \n",
    "   Hypothesis three: The Chehalis-350 treatment will have less Nitrate than the Chehalis-500 treatment.\n",
    "   \n",
    "2. In a markdown cell, write the mathematical hypotheses for each prediction  \n",
    "   Hypothesis 1:  H₀: μ₁=μ₂ ; Hₐ: μ₁>μ₂  \n",
    "   Hypothesis 2:  H₀: μ₁=μ₂ ; Hₐ: μ₁≠μ₂  \n",
    "   Hypothesis 3:  H₀: μ₁=μ₂ ; Hₐ: μ₁<μ₂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89af2b3b-c9aa-4ad2-828d-815c8d2cc561",
   "metadata": {},
   "source": [
    "### Part 7\n",
    "1. Choose a level of uncertainty (alpha), calculate the p-value (show your work), and compare your p-value to your alpha  \n",
    "   We will be testing hypothesis 2. alpha = 0.05 (the p value will be doubled to compare against this.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa05288c-e36e-48f9-92d8-0b18f5dcc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate p value\n",
    "def two_sample_t_test(sample_1,sample_2):\n",
    "    \"\"\"\n",
    "    This function performs compares the means of two\n",
    "    independent samples using a two-tailed t-test.\n",
    "    \n",
    "    It takes two samples as input.\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the mean of each sample\n",
    "    x_bar1 = np.mean(sample_1)\n",
    "    x_bar2 = np.mean(sample_2)\n",
    "    \n",
    "    # calculate the standard deviation of each \n",
    "    # sample using (n-1) in the denominator\n",
    "    s1 = np.std(sample_1, ddof=1)\n",
    "    s2 = np.std(sample_2, ddof=1)\n",
    "    \n",
    "    # get number of measurements in each sample\n",
    "    n1 = len(sample_1)\n",
    "    n2 = len(sample_2)\n",
    "    \n",
    "    # calculate the t-statistic\n",
    "    t = -1*abs( (x_bar1-x_bar2) / np.sqrt( ((s1**2)/n1) + ((s2**2)/n2) ) )\n",
    "    \n",
    "    # calculate degree of freedom\n",
    "    nu = n1+n2-2    \n",
    "    \n",
    "    # choose a method from above for calculating the t-statistic and p-value\n",
    "    # method 4\n",
    "    p = 2*st.t.cdf(t, nu)\n",
    "    \n",
    "    print(\"p-value: {}\".format(p))\n",
    "    \n",
    "    return t,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a560998c-bdb0-4e2e-8962-50ac5a7e1180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9202684418205275\n"
     ]
    }
   ],
   "source": [
    "# create the indexes for each sample\n",
    "idx_1 = df[\"Treatment\"] == \"Willamette.500\"\n",
    "idx_2 = df[\"Treatment\"] == \"Willamette.700\"\n",
    "\n",
    "sample_1 = df[idx_1][\"Nitrate\"]\n",
    "sample_2 = df[idx_2][\"Nitrate\"]\n",
    "t,p = two_sample_t_test(sample_1, sample_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2b3b0-d9b5-4834-820e-ae23a45937ae",
   "metadata": {},
   "source": [
    "Based on this high p value, we fail to reject the null hypothesis (0.92>0.05). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533bea99-fd28-438e-9831-97bfece07658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
